from typing import Optional, List
from ply import yacc

# AST NODES
from ..core.ast import (
    AstNode,
    Module,
)

from ..core.utils import Error

# LEXER / TOKENS
from ..lexer.tokens import TOKENS

# print(TOKENS)
from ..lexer.lexer import Lexer

# Parser Modules
from .parser_expressions import ExpressionRules
from .parser_statements import StatementRules
from .parser_loops import LoopRules
from .parser_conditionals import ConditionalRules
from .parser_definitions import DefinitionRules
from .parser_blocks import BlockRules

tokens = TOKENS  # tuple of tokens defined in the lexer

# PRECEDENCE
precedence = (
    ("left", "OR"),  # lowest
    ("left", "AND"),
    (
        "left",
        "EQUALS",
        "NOT_EQUALS",
        "LESS_THAN",
        "LESS_THAN_EQUALS",
        "GREATER_THAN",
        "GREATER_THAN_EQUALS",
    ),
    ("left", "PLUS", "MINUS"),
    ("left", "TIMES", "DIVIDE", "FLOOR_DIVIDE", "MOD"),
    ("right", "UPLUS", "UMINUS", "NOT"),
    ("right", "POWER"),  # highest
)


class Parser(
    ExpressionRules,
    StatementRules,
    LoopRules,
    ConditionalRules,
    DefinitionRules,
    BlockRules,
):
    """
    Implementation of the parser.
    It takes the source code tokenized by the lexer and converts it into an
    Abstract Syntax Tree(AST) that represents the structure of the code according to
    the defined grammatical rules. The parser uses PLY (Python Lex-Yacc) to build
    the tree from the tokens generated by the lexer.

    - Syntactically analyze language expressions and generate the corresponding AST.
    - Implement operator precedence rules and ensure that expressions are evaluated correctly.

    """

    tokens = TOKENS
    precedence = precedence

    def __init__(self, debug: bool = False):
        self.debug = debug
        self.errors: List[Error] = []
        self.data: Optional[str] = None

        # Lexer
        self.lexer = Lexer(errors=self.errors, debug=self.debug)
        self.lexer.build()

        self._parser = yacc.yacc(module=self, start="module", debug=self.debug)

    def parse(
        self, text: str
    ) -> (
        AstNode
    ):  # This function receives text and passes it to the lexer for tokenization.
        self.data = text
        self.lexer.input(text)
        # PLY takes tokens from self.lexer.lex
        try:
            result = self._parser.parse(lexer=self.lexer.lex, debug=self.debug)
            return result
        except SyntaxError as e:
            raise
        except Exception as e:
            self.errors.append(Error(f"Parser system error: {str(e)}", 0, 0, "parser", self.data))
            raise

    # ---------------------- MODULE ----------------------
    # Parse a module: top-level container of statements
    def p_module(self, p):
        """module : statement_list"""
        p[0] = Module(body=p[1], line=1, col=0)

    # ---------------------- STATEMENT LIST ----------------------
    # Parse a list of statements recursively
    # Can be a single statement or multiple statements
    def p_statement_list(self, p):
        """statement_list : statement
        | statement_list statement"""
        if len(p) == 2:
            p[0] = [p[1]]
        else:
            p[0] = p[1] + [p[2]]

    # ERRORS
    def p_error(self, t):
        if t is None:
            self.errors.append(Error("Unexpected end of input", 0, 0, "parser", self.data))
            return

        line_start = self.data.rfind("\n", 0, t.lexpos) + 1
        column = t.lexpos - line_start

        self.errors.append(
            Error(
                f"Syntax error near '{t.value}'",
                t.lineno,
                column,
                "parser",
                self.data
            )
        )

        self._parser.errok()