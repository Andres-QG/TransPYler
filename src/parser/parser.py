from typing import Optional, List
from ply import yacc

# AST NODES
from ..core.ast import (
    AstNode,
    Module,
)

from ..core.utils import Error

# LEXER / TOKENS
from ..lexer.tokens import TOKENS

# print(TOKENS)
from ..lexer.lexer import Lexer

# Parser Modules
from .parser_expressions import ExpressionRules
from .parser_statements import StatementRules
from .parser_loops import LoopRules
from .parser_conditionals import ConditionalRules
from .parser_definitions import DefinitionRules
from .parser_blocks import BlockRules

tokens = TOKENS  # tuple of tokens defined in the lexer

# PRECEDENCE
precedence = (
    ("left", "OR"),  # lowest
    ("left", "AND"),
    (
        "left",
        "EQUALS",
        "NOT_EQUALS",
        "LESS_THAN",
        "LESS_THAN_EQUALS",
        "GREATER_THAN",
        "GREATER_THAN_EQUALS",
    ),
    ("left", "PLUS", "MINUS"),
    ("left", "TIMES", "DIVIDE", "FLOOR_DIVIDE", "MOD"),
    (
        "right",
        "UPLUS",
        "UMINUS",
        "NOT",
    ),  # TODO(David): UPLUS and UMINUS are not implemented by lexer at least
    ("right", "POWER"),  # highest
)


class Parser(
    ExpressionRules,
    StatementRules,
    LoopRules,
    ConditionalRules,
    DefinitionRules,
    BlockRules,
):
    """
    Implementation of the parser.
    It takes the source code tokenized by the lexer and converts it into an
    Abstract Syntax Tree(AST) that represents the structure of the code according to
    the defined grammatical rules. The parser uses PLY (Python Lex-Yacc) to build
    the tree from the tokens generated by the lexer.

    - Syntactically analyze language expressions and generate the corresponding AST.
    - Implement operator precedence rules and ensure that expressions are evaluated correctly.

    """

    tokens = TOKENS
    precedence = precedence

    def __init__(self, debug: bool = False):
        self.debug = debug
        self.errors: List[Error] = []
        self.data: Optional[str] = None

        # Lexer shares the same error list, so both lexer and parser
        # can accumulate errors in one pass
        self.lexer = Lexer(errors=self.errors, debug=self.debug)
        self.lexer.build()

        self._parser = yacc.yacc(module=self, start="module", debug=self.debug)

    def parse(self, text: str) -> AstNode:
        """
        Parse source text and return an Abstract Syntax Tree.

        This function receives text and passes it to the lexer for tokenization,
        then uses PLY's parser to build the AST according to the grammar rules.

        Args:
            text: Source code string to parse

        Returns:
            AstNode: Root of the Abstract Syntax Tree (Module node)

        Note:
            Errors (both lexical and syntactic) are accumulated in self.errors
            rather than raising exceptions, allowing multiple errors to be
            reported in a single pass.
        """
        self.data = text
        self.lexer.input(text)

        # PLY takes tokens from self.lexer.lex
        # Note: We don't catch exceptions here because p_error handles syntax errors
        # without raising. Any exception that escapes is a serious bug.
        result = self._parser.parse(lexer=self.lexer.lex, debug=self.debug)

        return result

    # ---------------------- MODULE ----------------------
    # Parse a module: top-level container of statements
    def p_module(self, p):
        """module : statement_list"""
        p[0] = Module(body=p[1], line=1, col=0)

    # ---------------------- STATEMENT LIST ----------------------
    # Parse a list of statements recursively
    # Can be a single statement or multiple statements
    def p_statement_list(self, p):
        """statement_list : statement
        | statement_list statement"""
        if len(p) == 2:
            p[0] = [p[1]]
        else:
            p[0] = p[1] + [p[2]]

    # ERRORS catching
    # This function is called by PLY when a syntax error is encountered
    def p_error(self, t):
        """Error rule for syntax errors."""
        if t is None:
            self.errors.append(
                Error(
                    "Unexpected end of input while parsing", 0, 0, "parser", self.data
                )
            )
            return

        error_msg = f"Syntax error near '{t.value}'"

        if t.type == "INDENT":
            error_msg = "Unexpected indentation"
        elif t.type == "DEDENT":
            error_msg = "Unexpected dedentation (unindent does not match)"
        elif t.type in ("RPAREN", "RBRACKET", "RBRACE"):
            error_msg = f"Unexpected closing delimiter '{t.value}' - missing opening or extra closing"
        elif t.type == "COLON":
            error_msg = (
                "Unexpected ':' - check if/while/for/def/class syntax is correct"
            )
        elif t.type == "COMMA":
            error_msg = "Unexpected ',' - check list/tuple/function argument syntax"
        elif t.type in ("ASSIGN", "PLUS_ASSIGN", "MINUS_ASSIGN"):
            error_msg = f"Unexpected assignment operator '{t.value}' - missing target"
        elif t.type == "NEWLINE":
            error_msg = "Unexpected newline - statement may be incomplete"
        elif t.type in ("PLUS", "MINUS", "TIMES", "DIVIDE", "MOD", "POWER"):
            error_msg = f"Unexpected operator '{t.value}' - missing operand"

        self.errors.append(Error(error_msg, t.lineno, t.lexpos, "parser", self.data))

        while True:
            tok = self._parser.token()
            if not tok:
                break
            if tok.type in (
                "NEWLINE",
                "DEDENT",
                "IF",
                "WHILE",
                "FOR",
                "DEF",
                "CLASS",
                "RETURN",
            ):
                self._parser.errok()
                return tok

        self._parser.errok()
