from ply import yacc
from typing import Optional, List

# LEXER / TOKENS
from ..lexer.tokens import TOKENS

# print(TOKENS)
from ..lexer.lexer import Lexer

# Parser Modules

from .parser_expressions import ExpressionRules
from .parser_statements import StatementRules
from .parser_loops import LoopRules
from .parser_conditionals import ConditionalRules
from .parser_definitions import DefinitionRules
from .parser_blocks import BlockRules

tokens = TOKENS  # tuple of tokens defined in the lexer

# AST NODES
from ..core.ast import (
    AstNode,
    Module,
)

from ..core.utils import Error

# PRECEDENCE
precedence = (
    ("left", "OR"),  # lowest
    ("left", "AND"),
    (
        "left",
        "EQUALS",
        "NOT_EQUALS",
        "LESS_THAN",
        "LESS_THAN_EQUALS",
        "GREATER_THAN",
        "GREATER_THAN_EQUALS",
    ),
    ("left", "PLUS", "MINUS"),
    ("left", "TIMES", "DIVIDE", "FLOOR_DIVIDE", "MOD"),
    ("right", "UPLUS", "UMINUS", "NOT"),
    ("right", "POWER"),  # highest
)




class Parser (ExpressionRules, StatementRules, LoopRules, ConditionalRules, DefinitionRules, BlockRules):
    """
    Implementation of the parser.
    It takes the source code tokenized by the lexer and converts it into an
    Abstract Syntax Tree(AST) that represents the structure of the code according to
    the defined grammatical rules. The parser uses PLY (Python Lex-Yacc) to build
    the tree from the tokens generated by the lexer.

    - Syntactically analyze language expressions and generate the corresponding AST.
    - Implement operator precedence rules and ensure that expressions are evaluated correctly.

    """

    tokens = TOKENS
    precedence = precedence

    def __init__(self, debug: bool = False):
        self.debug = debug
        self.errors: List[Error] = []
        self.data: Optional[str] = None

        # Lexer
        self.lexer = Lexer(errors=self.errors, debug=self.debug)
        self.lexer.build()

        self._parser = yacc.yacc(module=self, start="module", debug=self.debug)

    def parse(
        self, text: str
    ) -> (
        AstNode
    ):  # This function receives text and passes it to the lexer for tokenization.
        self.data = text
        if not text.endswith("\n"):
            text += "\n"
        self.lexer.input(text)
        # PLY takes tokens from self.lexer.lex
        return self._parser.parse(lexer=self.lexer.lex, debug=self.debug)

    # ---------------------- MODULE ----------------------
    # Parse a module: top-level container of statements
    def p_module(self, p):
        """module : statement_list"""
        p[0] = Module(body=p[1], line=1, col=0)
        


    # ---------------------- STATEMENT LIST ----------------------
    # Parse a list of statements recursively
    # Can be a single statement or multiple statements
    def p_statement_list(self, p):
        """statement_list : statement
                        | statement_list statement"""
        if len(p) == 2:
            p[0] = [p[1]]
        else:
            p[0] = p[1] + [p[2]]

    # ERRORS
    def p_error(self, t):
        if t is None:
            self.errors.append(
                Error(
                    "Unexpected end of input while parsing", 0, 0, "parser", self.data
                )
            )
            return
        # Record the error
        self.errors.append(
            Error(
                f"Syntax error near '{t.value}'",
                t.lineno,
                t.lexpos,
                "parser",
                self.data,
            )
        )
        raise SyntaxError(self.errors[-1].exact())
